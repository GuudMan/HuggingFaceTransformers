from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd

# 1、read csv data
data = pd.read_csv("./data/ChnSentiCorp_htl_all.csv")
# print(data)
"""
      label                                             review
0         1  距离川沙公路较近,但是公交指示不对,如果是"蔡陆线"的话,会非常麻烦.建议用别的路线.房间较...
1         1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!
2         1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。
3         1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...
4         1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风
[7766 rows x 2 columns]
"""
data = data.dropna()
# print(data)
"""
[7765 rows x 2 columns]
"""

# 2、 create dataset
from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self) -> None:
        super().__init__()
        self.data = pd.read_csv("./data/ChnSentiCorp_htl_all.csv")
        self.data = self.data.dropna()
    
    def __getitem__(self, index):
        # return review and label respectively
        return self.data.iloc[index]["review"], self.data.iloc[index]['label']
    
    def __len__(self):
        return len(self.data)


dataset = MyDataset()
for i in range(5):
    print(dataset[i])
"""
('距离川沙公路较近,但是公交指示不对,如果是"蔡陆线"的话,会非常麻烦.建议用别的路线.房间较为简单.', 1)
('商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!', 1)
('早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。', 1)
('宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小，但加上低价位因素，还是无超所值的；环境不错，就在小胡同内，安静整洁，暖气好足-_-||。。。呵还有一大优势就是从宾馆出发，步行不到十分钟就可以到梅兰芳故居等等，京味小胡同，北海距离好近呢。总之，不错。推荐给节约消费的自助游朋友~比较划算，附近特色小吃很多~', 1)
('CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风', 1)
"""

# 3、dataset split
from torch.utils.data import random_split
trainset, validset = random_split(dataset, lengths=[0.9, 0.1])
print(len(trainset), len(validset))
"""
6989 776
"""

# 4、create dataloader
import torch
tokenizer = AutoTokenizer.from_pretrained("hfl/rbt3")

def collate_func(batch):
    texts, labels = [], []
    for item in batch:
        texts.append(item[0])
        labels.append(item[1])
    inputs = tokenizer(texts, max_length=128, padding="max_length", 
                       truncation=True,  return_tensors="pt")
    inputs["labels"] = torch.tensor(labels)
    return inputs

from torch.utils.data import DataLoader

# 4.1 without collate_func
# trainloader = DataLoader(trainset, batch_size=4, shuffle=True)

# print(next(enumerate(trainloader))[1])
"""
[('饭店较新,服务不错,旁边有小便利店,就是房价偏贵..', 
'房间比较大,但是设备显陈旧,最不喜欢是蹲厕,另外每层楼都是出租的办公室和酒店客房一起', 
'我是7月上旬去过的，酒店的环境很适合度假。山清水秀，幽静非常。但房间就不敢恭维；一进房首先是地毯的霉味很大很大（我要求换房可是很多标间都一样），
令人顶不顺。再有是灯光很暗，和房门锁不牢。还好服务员的服务意识和态度不错。值得安慰', 
'此次我在雅客滨江住了13个晚上，订的是城市景观房的高层。以下是我的感受：一，在上海这是一家性价比非常高的酒店。
房间布置简洁舒适，卫生间也很大，最令人开心的是有阳台。二,厨房设备齐全，冰箱很大，厨具餐具也比较全，
对于家庭旅行来说是非常实用的选择。三，服务很好，尤其是整理房间的工作人员，尽职尽责；认真热情。四，交通有些不方便，
但是附近有公交车的终点站，如果外地的朋友能仔细研究一下上海交通旅游图的话，会发现就附近的公交车站和地铁站就完全
可以满足你出行的需要了。总的来说我们这次入住这家酒店还是很满意的！！！'),
 tensor([1, 0, 1, 1])]
"""

# 4.2 customized collate_func
trainloader = DataLoader(trainset, batch_size=32, shuffle=True, 
                         collate_fn=collate_func)

print(next(enumerate(trainloader))[1])
"""
{'input_ids': tensor([[ 101,  831, 4157, 8038,  122, 8024,  769, 6858, 3683, 6772, 3175,  912,
         8024, 4895,  704, 2255, 1266, 6662, 1765, 7188, 4991, 3635, 6121,  126,
         1146, 7164,  511,  123, 8024, 3193, 7623, 3683, 6772,  705, 2168, 5375,
         4157, 8038,  122, 8024, 7564, 2137, 4638, 4294,  817, 2791, 8024, 1057,
          857, 3198, 5314, 2769, 2128, 2961, 1168,  671,  702, 1288, 7463, 4638,
         3517, 8020, 2791, 7313, 1920, 6956, 1146, 7463, 1762, 1912, 7481, 8021,
         8024, 2697, 6230,  679, 1922, 2128, 1059,  511,  123,  510, 1310, 4495,
         8024, 1057,  857, 5018,  676, 1921, 3198, 8024, 2791, 7313, 7305, 1366,
         4692, 6224,  671, 1372, 3647, 6096, 6082, 8024, 2791, 7313, 7027, 7607,
         4708,  123, 1372, 5721, 6066,  511, 1728, 3634, 2940,  749, 2791, 7313,
          511,  124, 8024, 3302, 1218,  679, 1922,  102],
        [ 101, 3241,  677, 3796, 3946, 3787,  722, 1400, 2347, 6814, 1061, 4157,
         8024, 1372, 5543, 1916, 6848, 2885, 1762, 6983, 2421, 4638,  704, 7623,
         1324, 1391, 7649, 8024, 3301, 1351, 1469, 2157,  782, 2190, 5831, 1501,
         4638, 1456, 6887, 6614,  679, 5318, 1366, 8024, 1377,  809, 6432, 3221,
         2769,  812,  969, 3189, 1139, 3952, 2792, 3297, 2692, 2682,  679, 1168,
         4638, 3119, 5815, 8024, 2792,  809, 4294, 1166, 1762, 6821, 7027, 2190,
         6983, 2421,  976, 1139, 7961, 1225, 8013,  102,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0],
        [ 101, 1922, 4060,  749, 8024, 3418, 3315,  679, 3221,  782,  857, 4638,
         8013,  817,  855,  948, 3221,  679, 7770, 8024,  852, 3221, 3025, 4923,
          817, 3766, 3300, 4993,  751, 1213, 8013,  102,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0],
        [ 101, 7579, 1912, 6413, 8038, 3341, 2769,  812, 2110, 3413, 2875, 5470,
         4638, 1296,  855, 1920, 6963,  857, 1762, 3173, 2643, 2772, 5442,  691,
         5526, 8024, 2523, 1914, 1398, 2110, 6963, 4916, 3353,  712, 1220, 4638,
         1343, 2875, 5470,  782, 1447, 2233,  857, 4638, 6983, 2421,  751, 1357,
         2339,  868, 3322,  833,  511, 1762, 3173, 2643, 8024, 2339,  868,  782,
         1447, 1762,  671, 3517, 4510, 3461, 1905, 1741, 6841, 1843, 2779, 8024,
         1780, 1104,  679, 1038, 6387, 2110, 4495,  733, 1777, 4510, 3461, 8024,
          754, 3221, 1920, 2157, 6963, 4260, 1282, 1126, 2231, 4638, 3517, 3461,
          511, 1762,  691, 5526, 8024, 2339,  868,  782, 1447, 1372, 3221, 2990,
         7008, 1920, 2157,  679, 6206, 2881, 2915, 8024,  809, 1048, 4510, 3461,
         5387, 2339, 8024, 1920, 2157, 6963, 2533,  102]]), 
         'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 
         'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1]]), 
         'labels': tensor([0, 1, 0, 1])}
"""


validloader = DataLoader(validset, batch_size=64, shuffle=False, 
                         collate_fn=collate_func)


# 5、 create model and optimizer
from torch.optim import Adam
model = AutoModelForSequenceClassification.from_pretrained("hfl/rbt3")
if torch.cuda.is_available():
    model = model.cuda()
optimizer = Adam(model.parameters(), lr=2e-5)


# 6、 train and evaluate

def evaluate():
    model.eval()
    acc_num = 0
    with torch.inference_mode():
        for batch in validloader:
            if torch.cuda.is_available():
                batch = {k:v.cuda() for k, v in batch.items()}
            output = model(**batch)
            pred = torch.argmax(output.logits, dim=-1)
            acc_num += (pred.long() == batch["labels"].long()).float().sum()
    return acc_num / len(validset)

def train(epoch=20, log_step=100):
    global_step = 0
    best_acc = 0.0
    for ep in range(epoch):
        model.train()
        for batch in trainloader:
            if torch.cuda.is_available():
                batch = {k: v.cuda() for k, v in batch.items()}
            optimizer.zero_grad()
            output = model(**batch)
            output.loss.backward()
            optimizer.step()
            if global_step % log_step == 0:
                print(f"ep: {ep}, global_step:{global_step}, loss:{output.loss.item()}")
            global_step += 1
        
        acc = evaluate()
        if acc > best_acc:
            best_acc = acc
            model.save_pretrained("rbt3_tx_cls")
            tokenizer.save_pretrained("rbt3_tx_cls")
        print(f"ep:{ep}, acc:{acc}")

train()

# 7、inference
infer_sen = "这家太远了"
id2_label = {0: "差评", 1: "好评"}
model.eval()
with torch.inference_mode():
    inputs = tokenizer(infer_sen, return_tensors="pt")
    inputs = {k:v.cuda() for k, v in inputs.items()}
    logits = model(**inputs).logits
    pred = torch.argmax(logits, dim=-1)
    print("预测结果：", id2_label.get(pred.item()))

